{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 2B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bLhECf3mbVtU",
    "outputId": "6ef79216-b77b-4a30-a3ea-9774c96d06e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: contractions in c:\\users\\manik\\anaconda3\\lib\\site-packages (0.0.18)\n"
     ]
    }
   ],
   "source": [
    "!pip install contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CP4Yjyt7bEXV",
    "outputId": "a23d1057-a15a-4226-b92d-21b44e32d1a1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\manik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\manik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\manik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Importing required libraries\n",
    "\n",
    "import re\n",
    "import contractions\n",
    "import nltk\n",
    "import numpy as np\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk import tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "\n",
    "txt = \"This movie made it into one of my top 10 most awful movies. Horrible. There wasn't a continuous minute where there wasn't a fight with one monster or another. There was no chance for any character development, they were too busy running from one sword fight to another. I had no emotional attachment ( except to the big bad machine ## that wanted to destroy them)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "VqWz4GLTcQJr"
   },
   "outputs": [],
   "source": [
    "class PreProcess():\n",
    "    \n",
    "    def __init__(self,text):\n",
    "        self.text = text\n",
    "        self.tokenized = []\n",
    "        self.stop_word = []\n",
    "        self.tags = []\n",
    "        self.expanded_txt = self.remcontra(self.text)\n",
    "        self.spec = self.remSpecChar(self.expanded_txt)\n",
    "\n",
    "    def remcontra(self,text:str):\n",
    "\n",
    "      '''\n",
    "          This method expands the contractions in the given text\n",
    "              Input arguments : text (Here the input is the text and this function expands the contractions inside the text \n",
    "              ---------------         For Ex: doesn't changes to does not)\n",
    "              Returns: We get the expanded text\n",
    "              --------\n",
    "              '''\n",
    "      words = []    \n",
    "      for word in text.split():\n",
    "        words.append(contractions.fix(word))   \n",
    "        expanded_text = ' '.join(words)\n",
    "      return expanded_text\n",
    "      \n",
    "\n",
    "    def remSpecChar(self, text:str):\n",
    "      '''This method removes the special characters along with extra spaces\n",
    "          Input arguments: text ( In this function all the special characters will be removed and extra spaces will be cleared )\n",
    "          ---------------     \n",
    "          Returns: The final text will have cleared special characters.'''\n",
    "\n",
    "      specChar = re.sub('[^A-Za-z0-9@]+',' ',text)\n",
    "      return specChar \n",
    "      \n",
    "\n",
    "    def regexp(self):\n",
    "      ''' \n",
    "          This method simplifies the digits and emails using regular expressions.\n",
    "                Input arguments: text (In this function the digits and email will be cleared by using certain patterns)\n",
    "                ---------------     \n",
    "                Returns:  We get simplified text.\n",
    "          '''\n",
    "      self.rg=re.sub('[\\w\\-\\.]+@([\\w-]+\\.)+[\\w-]{2,4}','email',self.spec)\n",
    "      self.rg=re.sub('\\d+','',self.rg)\n",
    "      return self.rg\n",
    "\n",
    "    def token(self,tokenize_on):\n",
    "\n",
    "      ''' In this function the whole text will be divided into tokens.\n",
    "                  By tokenization we can transform indivisible assets into tokens'''\n",
    "\n",
    "      self.tokenized = nltk.word_tokenize(self.rg)\n",
    "      return self.tokenized\n",
    "\n",
    "    def remove_stopwords(self):\n",
    "      ''' \n",
    "        Stopwords are a set of commonly used words such as is, the, are etc.. \n",
    "        Input\n",
    "        -----\n",
    "        text file as string\n",
    "        \n",
    "        Returns\n",
    "        ------\n",
    "        list of words without stopwords\n",
    "      '''\n",
    "      stop_words = set(stopwords.words('english'))  \n",
    "      for w in self.tokenized:\n",
    "        if w notn stop_words:\n",
    "          self.stop_word.append(w)\n",
    "      return self.stop_word\n",
    "      \n",
    "    def stemmingOrLemmatization(self, method):\n",
    "      \"\"\"\n",
    "      stemmingOrLemmatization(argument) function decides which functions should\n",
    "      be running based on the input\n",
    "      \"\"\"\n",
    "      if method == 'stem':\n",
    "        self.out = self.stemming()\n",
    "      else:\n",
    "        self.out = self.lemmatization()\n",
    "      return self.out\n",
    "\n",
    "    # Stemming\n",
    "\n",
    "\n",
    "    def stemming(self):\n",
    "      \"\"\"\n",
    "      Stemming() removes suffix from a word and reduce it to \n",
    "      its root word.\n",
    "      \"\"\"\n",
    "      stm = nltk.porter.PorterStemmer()\n",
    "      stword = [stm.stem(word) for word in self.stop_word]\n",
    "      return stword\n",
    "    \n",
    "    # Lemmetization\n",
    "    def lemmatization(self):\n",
    "      \"\"\"\n",
    "      lemmatization() functions takes input from stemming and reduces it to the\n",
    "      right word\n",
    "      \"\"\"\n",
    "      lem = WordNetLemmatizer()\n",
    "      lemout = [lem.lemmatize(word) for word in self.stop_word]\n",
    "      return lemout\n",
    "        \n",
    "        \n",
    "    def ngram(self,tx):\n",
    "      ''' \n",
    "      In this function it returns a sequence of N items from a given sample of text.\n",
    "      Here an item can be a character,words,sentence and N can be any integer\n",
    "              \n",
    "      '''\n",
    "      NGRAMS=ngrams(sequence=nltk.word_tokenize(tx), n=5)\n",
    "      for grams in NGRAMS:\n",
    "          print(grams)\n",
    "\n",
    "      \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "id": "M7m61AjscZko",
    "outputId": "cfbfaec6-3249-4ce6-d45e-84aa54f3a9ae"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This movie made it into one of my top  most awful movies Horrible There was not a continuous minute where there was not a fight with one monster or another There was no chance for any character development they were too busy running from one sword fight to another I had no emotional attachment except to the big bad machine that wanted to destroy them '"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process = PreProcess(txt)\n",
    "process.regexp()\n",
    "#process.getProcessedData()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ULP4-RfGf5gk",
    "outputId": "35ca9e9b-fe7b-4a30-d9f5-04b1a8a76dac"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'movie',\n",
       " 'made',\n",
       " 'it',\n",
       " 'into',\n",
       " 'one',\n",
       " 'of',\n",
       " 'my',\n",
       " 'top',\n",
       " 'most',\n",
       " 'awful',\n",
       " 'movies',\n",
       " 'Horrible',\n",
       " 'There',\n",
       " 'was',\n",
       " 'not',\n",
       " 'a',\n",
       " 'continuous',\n",
       " 'minute',\n",
       " 'where',\n",
       " 'there',\n",
       " 'was',\n",
       " 'not',\n",
       " 'a',\n",
       " 'fight',\n",
       " 'with',\n",
       " 'one',\n",
       " 'monster',\n",
       " 'or',\n",
       " 'another',\n",
       " 'There',\n",
       " 'was',\n",
       " 'no',\n",
       " 'chance',\n",
       " 'for',\n",
       " 'any',\n",
       " 'character',\n",
       " 'development',\n",
       " 'they',\n",
       " 'were',\n",
       " 'too',\n",
       " 'busy',\n",
       " 'running',\n",
       " 'from',\n",
       " 'one',\n",
       " 'sword',\n",
       " 'fight',\n",
       " 'to',\n",
       " 'another',\n",
       " 'I',\n",
       " 'had',\n",
       " 'no',\n",
       " 'emotional',\n",
       " 'attachment',\n",
       " 'except',\n",
       " 'to',\n",
       " 'the',\n",
       " 'big',\n",
       " 'bad',\n",
       " 'machine',\n",
       " 'that',\n",
       " 'wanted',\n",
       " 'to',\n",
       " 'destroy',\n",
       " 'them']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process.token(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MJhXVaRztyBL",
    "outputId": "3683b3da-30ed-4b18-e8a8-12fea91fd954"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'movie',\n",
       " 'made',\n",
       " 'one',\n",
       " 'top',\n",
       " 'awful',\n",
       " 'movies',\n",
       " 'Horrible',\n",
       " 'There',\n",
       " 'continuous',\n",
       " 'minute',\n",
       " 'fight',\n",
       " 'one',\n",
       " 'monster',\n",
       " 'another',\n",
       " 'There',\n",
       " 'chance',\n",
       " 'character',\n",
       " 'development',\n",
       " 'busy',\n",
       " 'running',\n",
       " 'one',\n",
       " 'sword',\n",
       " 'fight',\n",
       " 'another',\n",
       " 'I',\n",
       " 'emotional',\n",
       " 'attachment',\n",
       " 'except',\n",
       " 'big',\n",
       " 'bad',\n",
       " 'machine',\n",
       " 'wanted',\n",
       " 'destroy']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process.remove_stopwords()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cv8nWcHwwzm3",
    "outputId": "c9e74e67-6fbe-4b32-bbd2-9d94b6c83f51"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['thi',\n",
       " 'movi',\n",
       " 'made',\n",
       " 'one',\n",
       " 'top',\n",
       " 'aw',\n",
       " 'movi',\n",
       " 'horribl',\n",
       " 'there',\n",
       " 'continu',\n",
       " 'minut',\n",
       " 'fight',\n",
       " 'one',\n",
       " 'monster',\n",
       " 'anoth',\n",
       " 'there',\n",
       " 'chanc',\n",
       " 'charact',\n",
       " 'develop',\n",
       " 'busi',\n",
       " 'run',\n",
       " 'one',\n",
       " 'sword',\n",
       " 'fight',\n",
       " 'anoth',\n",
       " 'i',\n",
       " 'emot',\n",
       " 'attach',\n",
       " 'except',\n",
       " 'big',\n",
       " 'bad',\n",
       " 'machin',\n",
       " 'want',\n",
       " 'destroy']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process.stemmingOrLemmatization('stem')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ni5mWLgjxSK-",
    "outputId": "aabdc041-760b-4fb7-d70d-e5a27ec2215f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'movie',\n",
       " 'made',\n",
       " 'one',\n",
       " 'top',\n",
       " 'awful',\n",
       " 'movie',\n",
       " 'Horrible',\n",
       " 'There',\n",
       " 'continuous',\n",
       " 'minute',\n",
       " 'fight',\n",
       " 'one',\n",
       " 'monster',\n",
       " 'another',\n",
       " 'There',\n",
       " 'chance',\n",
       " 'character',\n",
       " 'development',\n",
       " 'busy',\n",
       " 'running',\n",
       " 'one',\n",
       " 'sword',\n",
       " 'fight',\n",
       " 'another',\n",
       " 'I',\n",
       " 'emotional',\n",
       " 'attachment',\n",
       " 'except',\n",
       " 'big',\n",
       " 'bad',\n",
       " 'machine',\n",
       " 'wanted',\n",
       " 'destroy']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process.stemmingOrLemmatization('lemm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ngramming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jtaXC91uxWpd",
    "outputId": "d86234de-958e-44ef-fa4f-b1313cf03d08"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('This', 'movie', 'made', 'it', 'into')\n",
      "('movie', 'made', 'it', 'into', 'one')\n",
      "('made', 'it', 'into', 'one', 'of')\n",
      "('it', 'into', 'one', 'of', 'my')\n",
      "('into', 'one', 'of', 'my', 'top')\n",
      "('one', 'of', 'my', 'top', '10')\n",
      "('of', 'my', 'top', '10', 'most')\n",
      "('my', 'top', '10', 'most', 'awful')\n",
      "('top', '10', 'most', 'awful', 'movies')\n",
      "('10', 'most', 'awful', 'movies', '.')\n",
      "('most', 'awful', 'movies', '.', 'Horrible')\n",
      "('awful', 'movies', '.', 'Horrible', '.')\n",
      "('movies', '.', 'Horrible', '.', 'There')\n",
      "('.', 'Horrible', '.', 'There', 'was')\n",
      "('Horrible', '.', 'There', 'was', \"n't\")\n",
      "('.', 'There', 'was', \"n't\", 'a')\n",
      "('There', 'was', \"n't\", 'a', 'continuous')\n",
      "('was', \"n't\", 'a', 'continuous', 'minute')\n",
      "(\"n't\", 'a', 'continuous', 'minute', 'where')\n",
      "('a', 'continuous', 'minute', 'where', 'there')\n",
      "('continuous', 'minute', 'where', 'there', 'was')\n",
      "('minute', 'where', 'there', 'was', \"n't\")\n",
      "('where', 'there', 'was', \"n't\", 'a')\n",
      "('there', 'was', \"n't\", 'a', 'fight')\n",
      "('was', \"n't\", 'a', 'fight', 'with')\n",
      "(\"n't\", 'a', 'fight', 'with', 'one')\n",
      "('a', 'fight', 'with', 'one', 'monster')\n",
      "('fight', 'with', 'one', 'monster', 'or')\n",
      "('with', 'one', 'monster', 'or', 'another')\n",
      "('one', 'monster', 'or', 'another', '.')\n",
      "('monster', 'or', 'another', '.', 'There')\n",
      "('or', 'another', '.', 'There', 'was')\n",
      "('another', '.', 'There', 'was', 'no')\n",
      "('.', 'There', 'was', 'no', 'chance')\n",
      "('There', 'was', 'no', 'chance', 'for')\n",
      "('was', 'no', 'chance', 'for', 'any')\n",
      "('no', 'chance', 'for', 'any', 'character')\n",
      "('chance', 'for', 'any', 'character', 'development')\n",
      "('for', 'any', 'character', 'development', ',')\n",
      "('any', 'character', 'development', ',', 'they')\n",
      "('character', 'development', ',', 'they', 'were')\n",
      "('development', ',', 'they', 'were', 'too')\n",
      "(',', 'they', 'were', 'too', 'busy')\n",
      "('they', 'were', 'too', 'busy', 'running')\n",
      "('were', 'too', 'busy', 'running', 'from')\n",
      "('too', 'busy', 'running', 'from', 'one')\n",
      "('busy', 'running', 'from', 'one', 'sword')\n",
      "('running', 'from', 'one', 'sword', 'fight')\n",
      "('from', 'one', 'sword', 'fight', 'to')\n",
      "('one', 'sword', 'fight', 'to', 'another')\n",
      "('sword', 'fight', 'to', 'another', '.')\n",
      "('fight', 'to', 'another', '.', 'I')\n",
      "('to', 'another', '.', 'I', 'had')\n",
      "('another', '.', 'I', 'had', 'no')\n",
      "('.', 'I', 'had', 'no', 'emotional')\n",
      "('I', 'had', 'no', 'emotional', 'attachment')\n",
      "('had', 'no', 'emotional', 'attachment', '(')\n",
      "('no', 'emotional', 'attachment', '(', 'except')\n",
      "('emotional', 'attachment', '(', 'except', 'to')\n",
      "('attachment', '(', 'except', 'to', 'the')\n",
      "('(', 'except', 'to', 'the', 'big')\n",
      "('except', 'to', 'the', 'big', 'bad')\n",
      "('to', 'the', 'big', 'bad', 'machine')\n",
      "('the', 'big', 'bad', 'machine', '#')\n",
      "('big', 'bad', 'machine', '#', '#')\n",
      "('bad', 'machine', '#', '#', 'that')\n",
      "('machine', '#', '#', 'that', 'wanted')\n",
      "('#', '#', 'that', 'wanted', 'to')\n",
      "('#', 'that', 'wanted', 'to', 'destroy')\n",
      "('that', 'wanted', 'to', 'destroy', 'them')\n",
      "('wanted', 'to', 'destroy', 'them', ')')\n"
     ]
    }
   ],
   "source": [
    "process.ngram(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "data = [ process.regexp() ]\n",
    "tfidf = TfidfVectorizer()\n",
    "result = tfidf.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word indexes:\n",
      "{'this': 41, 'movie': 26, 'made': 22, 'it': 20, 'into': 19, 'one': 32, 'of': 31, 'my': 28, 'top': 44, 'most': 25, 'awful': 3, 'movies': 27, 'horrible': 18, 'there': 39, 'was': 46, 'not': 30, 'continuous': 9, 'minute': 23, 'where': 48, 'fight': 14, 'with': 49, 'monster': 24, 'or': 33, 'another': 0, 'no': 29, 'chance': 7, 'for': 15, 'any': 1, 'character': 8, 'development': 11, 'they': 40, 'were': 47, 'too': 43, 'busy': 6, 'running': 34, 'from': 16, 'sword': 35, 'to': 42, 'had': 17, 'emotional': 12, 'attachment': 2, 'except': 13, 'the': 37, 'big': 5, 'bad': 4, 'machine': 21, 'that': 36, 'wanted': 45, 'destroy': 10, 'them': 38}\n",
      "\n",
      "tf-idf values in matrix form:\n",
      "[[0.20628425 0.10314212 0.10314212 0.10314212 0.10314212 0.10314212\n",
      "  0.10314212 0.10314212 0.10314212 0.10314212 0.10314212 0.10314212\n",
      "  0.10314212 0.10314212 0.20628425 0.10314212 0.10314212 0.10314212\n",
      "  0.10314212 0.10314212 0.10314212 0.10314212 0.10314212 0.10314212\n",
      "  0.10314212 0.10314212 0.10314212 0.10314212 0.10314212 0.20628425\n",
      "  0.20628425 0.10314212 0.30942637 0.10314212 0.10314212 0.10314212\n",
      "  0.10314212 0.10314212 0.10314212 0.30942637 0.10314212 0.10314212\n",
      "  0.30942637 0.10314212 0.10314212 0.10314212 0.30942637 0.10314212\n",
      "  0.10314212 0.10314212]]\n"
     ]
    }
   ],
   "source": [
    "# get indexing\n",
    "print('\\nWord indexes:')\n",
    "print(tfidf.vocabulary_)\n",
    "  \n",
    "print('\\ntf-idf values in matrix form:')\n",
    "print(result.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "AML-2203_Project2B.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
