{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7b04ac6",
   "metadata": {},
   "source": [
    "# Project 2c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8996c1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing required libraries\n",
    "\n",
    "import re\n",
    "import contractions\n",
    "import numpy as np\n",
    "\n",
    "txt = \"This movie made it into one of my top 10 most awful movies. Horrible. There wasn't a continuous minute where there wasn't a fight with one monster or another. There was no chance for any character development, they were too busy running from one sword fight to another. I had no emotional attachment ( except to the big bad machine ## that wanted to destroy them)\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2fa1ac9",
   "metadata": {},
   "source": [
    "Cleaning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11ab4b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreProcess():\n",
    "    \n",
    "    def __init__(self,text):\n",
    "        self.text = text\n",
    "        self.tokenized = []\n",
    "        self.stop_word = []\n",
    "        self.tags = []\n",
    "        self.expanded_txt = self.remcontra(self.text)\n",
    "        self.spec = self.remSpecChar(self.expanded_txt)\n",
    "\n",
    "    def remcontra(self,text:str):\n",
    "\n",
    "      '''\n",
    "          This method expands the contractions in the given text\n",
    "              Input arguments : text (Here the input is the text and this function expands the contractions inside the text \n",
    "              ---------------         For Ex: doesn't changes to does not)\n",
    "              Returns: We get the expanded text\n",
    "              --------\n",
    "              '''\n",
    "      words = []    \n",
    "      for word in text.split():\n",
    "        words.append(contractions.fix(word))   \n",
    "        expanded_text = ' '.join(words)\n",
    "      return expanded_text\n",
    "      \n",
    "\n",
    "    def remSpecChar(self, text:str):\n",
    "      '''This method removes the special characters along with extra spaces\n",
    "          Input arguments: text ( In this function all the special characters will be removed and extra spaces will be cleared )\n",
    "          ---------------     \n",
    "          Returns: The final text will have cleared special characters.'''\n",
    "\n",
    "      specChar = re.sub('[^A-Za-z0-9@]+',' ',text)\n",
    "      return specChar \n",
    "      \n",
    "\n",
    "    def regexp(self):\n",
    "      ''' \n",
    "          This method simplifies the digits and emails using regular expressions.\n",
    "                Input arguments: text (In this function the digits and email will be cleared by using certain patterns)\n",
    "                ---------------     \n",
    "                Returns:  We get simplified text.\n",
    "          '''\n",
    "      self.rg=re.sub('[\\w\\-\\.]+@([\\w-]+\\.)+[\\w-]{2,4}','email',self.spec)\n",
    "      self.rg=re.sub('\\d+','',self.rg)\n",
    "      return self.rg\n",
    "\n",
    "    def token(self,tokenize_on):\n",
    "\n",
    "      ''' In this function the whole text will be divided into tokens.\n",
    "                  By tokenization we can transform indivisible assets into tokens'''\n",
    "\n",
    "      self.tokenized = nltk.word_tokenize(self.rg)\n",
    "      return self.tokenized\n",
    "\n",
    "    def remove_stopwords(self):\n",
    "      ''' \n",
    "        Stopwords are a set of commonly used words such as is, the, are etc.. \n",
    "        Input\n",
    "        -----\n",
    "        text file as string\n",
    "        \n",
    "        Returns\n",
    "        ------\n",
    "        list of words without stopwords\n",
    "      '''\n",
    "      stop_words = set(stopwords.words('english'))  \n",
    "      for w in self.tokenized:\n",
    "        if w not in stop_words:\n",
    "          self.stop_word.append(w)\n",
    "      return self.stop_word\n",
    "      \n",
    "    def stemmingOrLemmatization(self, method):\n",
    "      \"\"\"\n",
    "      stemmingOrLemmatization(argument) function decides which functions should\n",
    "      be running based on the input\n",
    "      \"\"\"\n",
    "      if method == 'stem':\n",
    "        self.out = self.stemming()\n",
    "      else:\n",
    "        self.out = self.lemmatization()\n",
    "      return self.out\n",
    "\n",
    "    # Stemming\n",
    "\n",
    "\n",
    "    def stemming(self):\n",
    "      \"\"\"\n",
    "      Stemming() removes suffix from a word and reduce it to \n",
    "      its root word.\n",
    "      \"\"\"\n",
    "      stm = nltk.porter.PorterStemmer()\n",
    "      stword = [stm.stem(word) for word in self.stop_word]\n",
    "      return stword\n",
    "    \n",
    "    # Lemmetization\n",
    "    def lemmatization(self):\n",
    "      \"\"\"\n",
    "      lemmatization() functions takes input from stemming and reduces it to the\n",
    "      right word\n",
    "      \"\"\"\n",
    "      lem = WordNetLemmatizer()\n",
    "      lemout = [lem.lemmatize(word) for word in self.stop_word]\n",
    "      return lemout\n",
    "        \n",
    "        \n",
    "    def ngram(self,tx):\n",
    "      ''' \n",
    "      In this function it returns a sequence of N items from a given sample of text.\n",
    "      Here an item can be a character,words,sentence and N can be any integer\n",
    "              \n",
    "      '''\n",
    "      NGRAMS=ngrams(sequence=nltk.word_tokenize(tx), n=5)\n",
    "      for grams in NGRAMS:\n",
    "          print(grams)\n",
    "\n",
    "      \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c543e82",
   "metadata": {},
   "source": [
    "Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56f84807",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This movie made it into one of my top  most awful movies Horrible There was not a continuous minute where there was not a fight with one monster or another There was no chance for any character development they were too busy running from one sword fight to another I had no emotional attachment except to the big bad machine that wanted to destroy them '"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "process = PreProcess(txt)\n",
    "process.regexp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "905473b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n"
     ]
    }
   ],
   "source": [
    "#Importing sklearn libraries\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction import _stop_words\n",
    "text = [process.regexp()]\n",
    "StopWords = list(_stop_words.ENGLISH_STOP_WORDS)\n",
    "vectorizer = CountVectorizer(stop_words=StopWords)\n",
    "vectorizer.fit(text)\n",
    "print(len(vectorizer.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d19bbc5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'movie': 16, 'awful': 1, 'movies': 17, 'horrible': 12, 'continuous': 7, 'minute': 14, 'fight': 11, 'monster': 15, 'chance': 5, 'character': 6, 'development': 9, 'busy': 4, 'running': 18, 'sword': 19, 'emotional': 10, 'attachment': 0, 'big': 3, 'bad': 2, 'machine': 13, 'wanted': 20, 'destroy': 8}\n"
     ]
    }
   ],
   "source": [
    "#Printing vocabulary\n",
    "\n",
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4197ef1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "#Vectors to Array\n",
    "\n",
    "vectors = vectorizer.transform(text)\n",
    "print(vectors.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701359f6",
   "metadata": {},
   "source": [
    "TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e57f8248",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This movie made it into one of my top  most awful movies Horrible There was not a continuous minute where there was not a fight with one monster or another There was no chance for any character development they were too busy running from one sword fight to another I had no emotional attachment except to the big bad machine that wanted to destroy them ']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importing TFIDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61351e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "result = tfidf.fit_transform(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f5fcdb",
   "metadata": {},
   "source": [
    "Giving Indexes and TFIDF Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a8bff6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word indexes:\n",
      "{'this': 41, 'movie': 26, 'made': 22, 'it': 20, 'into': 19, 'one': 32, 'of': 31, 'my': 28, 'top': 44, 'most': 25, 'awful': 3, 'movies': 27, 'horrible': 18, 'there': 39, 'was': 46, 'not': 30, 'continuous': 9, 'minute': 23, 'where': 48, 'fight': 14, 'with': 49, 'monster': 24, 'or': 33, 'another': 0, 'no': 29, 'chance': 7, 'for': 15, 'any': 1, 'character': 8, 'development': 11, 'they': 40, 'were': 47, 'too': 43, 'busy': 6, 'running': 34, 'from': 16, 'sword': 35, 'to': 42, 'had': 17, 'emotional': 12, 'attachment': 2, 'except': 13, 'the': 37, 'big': 5, 'bad': 4, 'machine': 21, 'that': 36, 'wanted': 45, 'destroy': 10, 'them': 38}\n",
      "\n",
      "tf-idf values in matrix form:\n",
      "[[0.20628425 0.10314212 0.10314212 0.10314212 0.10314212 0.10314212\n",
      "  0.10314212 0.10314212 0.10314212 0.10314212 0.10314212 0.10314212\n",
      "  0.10314212 0.10314212 0.20628425 0.10314212 0.10314212 0.10314212\n",
      "  0.10314212 0.10314212 0.10314212 0.10314212 0.10314212 0.10314212\n",
      "  0.10314212 0.10314212 0.10314212 0.10314212 0.10314212 0.20628425\n",
      "  0.20628425 0.10314212 0.30942637 0.10314212 0.10314212 0.10314212\n",
      "  0.10314212 0.10314212 0.10314212 0.30942637 0.10314212 0.10314212\n",
      "  0.30942637 0.10314212 0.10314212 0.10314212 0.30942637 0.10314212\n",
      "  0.10314212 0.10314212]]\n"
     ]
    }
   ],
   "source": [
    "# get indexing\n",
    "print('\\nWord indexes:')\n",
    "print(tfidf.vocabulary_)\n",
    "  \n",
    "print('\\ntf-idf values in matrix form:')\n",
    "print(result.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5069da08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
